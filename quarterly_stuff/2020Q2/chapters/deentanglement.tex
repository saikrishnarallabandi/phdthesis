\chapter{De-Entanglement}

\section{How to accomplish de-entanglement?}

The most popular approach to obtain isolation of factors of variation in neural models is by employing stochastic random variables. This approach provides flexibility to jointly train the latent representations as well as the downstream  network. It has been observed that the latent representations resemble disentangled representations under certain conditions \citep{isolating_sources_betavae, understanding_disentanglement_betavae,structured_disentangled_representations,hyperprior_disentanglement}. Note that although obtaining such degenerate representations is considered typical, it is not the only manifestation: it also manifests as continuous representations\citep{ravanelli2018interpretablesyncnet} and other abstract phenomena(e.g. grounding). I argue that explicitly controlling what and how much gets de-entangled \citep{understanding_disentanglement_betavae} is better than implicit disentanglement as is followed today\citep{locatello2018challenging}. I identify four ways to computationally control de-entanglement in encoder decoder models

\begin{itemize}
    \item (1) By employing suitable priors about task or data distribution
    \item (2) By incorporating additional adversarial or multi task objectives within the model
    \item (3) By utilizing a different divergence objective
    \item (4) By employing an alternative formulation of probability density estimation
\end{itemize}

I will  expand on each of these in the following chapters, providing one task from language technologies as a case study. 



I posit that designing learning paradigms such that we explicitly control de-entanglement of relevant factors of variation while marginalizing the nuisance factors of variation leads to massive improvements. Such an approach, I claim, leads to further advantages in the context of both generative processes: in terms of generation of novel content and discriminative processes: in terms of robustness of such models to noise and attacks. Let us consider a typical deep learning architecture such as AlexNet\citep{alexnet}. It is characterized by a series of convolutional layers (feature extraction module) followed by a pooling layer and a SoftMax layer(classification module). Note that while I mention AlexNet as an example, this abstraction can be extended to most sequence to sequence architectures with encoder as feature extraction module and decoder as the classification module\citep{tutorial_dataaugmentation} across modalities and tasks. It can be shown that the pooling layer acts as information bottleneck\citep{tishby2000information} module in such architectures. I point out\citep{variational_attention_rsk} that in case of conventional Seq2Seq architectures deployed today, attention plays the role of information bottleneck module regulating the amount of information being utilized by the decoder. In \citep{variational_attention_rsk,vyas2019learning} I show that this module controls optimization in encoder decoder models leading to (1) Disentanglement of Causal Factors of variation in the data distribution (2) Marginalization of nuisance factors of variation from the input distribution. In case of models that employ stochasticity, two more effects can be observed : (a) Posterior collapse or Degeneration due to powerful decoders and (b) Loss of output fidelity due to finite capacity decoders. In current architectures, marginalization and disentanglement are realized implicitly and often lead to (a) and (b) when deployed in practise.

The most popular approach to obtain isolation of factors of variation in neural models is by employing stochastic random variables. This approach provides flexibility to jointly train the latent representations as well as the downstream  network. It has been observed that the latent representations resemble disentangled representations under certain conditions \citep{isolating_sources_betavae, understanding_disentanglement_betavae,structured_disentangled_representations,hyperprior_disentanglement}. Note that although obtaining such degenerate representations is considered typical, it is not the only manifestation: it also manifests as continuous representations\citep{ravanelli2018interpretablesyncnet} and other abstract phenomena(e.g. grounding). I argue that explicitly controlling what and how much gets de-entangled \citep{understanding_disentanglement_betavae} is better than implicit disentanglement as is followed today\citep{locatello2018challenging}. I identify four ways to computationally control de-entanglement in encoder decoder models

\begin{itemize}
    \item (1) By employing suitable priors about task or data distribution
    \item (2) By incorporating additional adversarial or multi task objectives within the model
    \item (3) By utilizing a different divergence objective
    \item (4) By employing an alternative formulation of probability density estimation
\end{itemize}






\subsection{Case for Controlled De-Entanglement}

I believe that complete disentanglement of input data into its independent causal factors of variation is not fully useful. A more attractive option is to control what and how much gets de-entangled in a a task dependent manner. It has to be noted that given a particular downstream task, some causal factors of variation might not be relevant, in which case modeling them would be unnecessary. Let us consider a data distribution X which consists of class examples $\{ x_1, x_2...,x_n \}$, where each $x_i$ is described by attribute-set $(a,b,c)$. The prior distribution of X can be represented by a parameteric function g such that g maximizes the likelihood of X over the set of its attributes:

\begin{equation}
P_{\omega}(X) = g_{\omega}(a,b,c)
\end{equation}

Note that the attribute-set can either contain individual entities or the relationships between them or both.  To illustrate this, let us consider a toy-example where we build a binary classifier to predict if a given integer triplet is a Pythagorean triplet. Pythagorean triplets are a triplet of numbers that follow Pythagoras Theorem such as $\{3,4,5\}$ and $\{5,12,13 \}$. In this task, the attribute-set consists of the relationship between the first two-elements of the triplet. If the model is able to discover this attribute, it can generalize for any given numbers. %Another example is to build a generative model for predicting the next number in the Fibonacci series. The attribute-set consists of the relation between the last two elements on the input list. 
However, if we have a more complicated task like building a classifier for MNIST digits, then the attribute-set has multiple first and second order relations like brush strokes, shape of the digits etc. The success of modelling $P_{\omega}(X)$, and ultimately the success on the downstream task, relies on how well can the model isolate these individual attributes from the observed data $X_{t}$. This isolation ability becomes even more important in case we want to regenerate the digits using a generative model. Mathematically, let us consider the posterior probability of a training instance $x_1$ expressed as 


\begin{equation}
P_{\theta}(x_1) = f_{\theta}(x_1)
\end{equation}

where \textit{f} denotes arbitrary function and $\theta$ denotes the parametric family used to model the distribution $X$. It can be seen that compositionality over an unseen training instant $x_{new}$ would be possible if \textit{f} is related to \textit{g}. In other words, \textit{f} needs to intuitively have some information about the latent causal factors of variation that generated $X$ in the first place. In such scenarios, the test instance can be appropriately expressed as 

\begin{equation}
P_{\theta}(x_{new}) = h(a,k(b,c))
\end{equation}

where h and k can be a novel combination of functions that embed these attributes in the manifold of original distribution of $X$. Not tracking the relevant factors of variation typically leads to model memorizing only the surface level associations leading to mode collapse and lack of diversity in the generated outputs. On the other hand, explicitly caring about the factors of variation can be seen as a way of incorporating inductive bias into the model and has the potential to avoid such pitfalls. 
 
\iffalse 
\subsection{Inductive Bias}
Inductive bias typically refers to the set of constraints applied on the model that help restrict the hypothesis space apriori based on the background knowledge about the distribution of data. Having such priors helps the model reach the solution faster. For instance, consider the prior of linearity imposed on a curve fittign task between two points in cartesian co ordinates. Having such prior extremely simplifies the task since there is only one line through two points while there are infinitely many curves. Not having any sort of priors about the data distribution might also lead to uninterpretible and black box results even if the solution is infact correct. While this might be okay in the beginning, to make the technology closer to acceptance, we need models that are interpretible atleast conditionally. Not having any sort of inductive biases might lead to model memorizing or just learning surface level associations. This leads to the model becoming vulnerable to adversarial attacks. Voice impersonation attacks for example can be very serious. Additionally, having such priors in place also acts as a probe to let us diagnose the performance of the model when it fails. This we believe is a more principled way towards building and debugging reliable systems as opposed to post hoc fixes based on the errors made by such models. Community today is two paced: either based on end to end neural approaches where the aim is to discover everything through data or based on hand crafted features. We believe that a far more interesting approach is to take a leaf out of research from epigenetics that shows biology builds both by nature as well as nurture and not absolute. Specifically, we believe going forward it is important to design techniques that have quirks from the nature - prior knowledge incorporated into the nurture - end to end approaches. In this paper, we perform a case study on a manifestation of speech generation, voice conversion, and present approaches that enable us to incorporate inductive biases into the model.
\fi


\begin{center}
\begin{table}
\caption{Implicit Realization of information bottleneck in popular deep learning mechanisms}
\begin{tabular}{  | c | c | c| }
\hline
\textbf{Architecture} & \textbf{Manifestation of Information Bottleneck} & \textbf{Type of Bottleneck} \\
\hline
AlexNet & Pooling  & Spatial \\
 \hline 
Attention & Activation & Temporal\\
\hline
VAE & Priors & Spatio temporal\\
\hline
Neural Module Networks & Softmax over Modules & Spatial \\
\hline
LISA & Linguistic Priors & Temporal\\
\hline
BERT & Masking  & Spatio temporal\\ 
\hline
XLNet & Permutation & Spatio temporal \\
 \hline
\end{tabular}

\end{table}
\label{Message}
\end{center}


\subsection{Implicit De-Entanglement in Seq2Seq Models: Deterministic Attention vs Stochastic Attention}

I will illustrate this sub section with a typical generative model of speech: Text to Speech. Consider that we are interested in building a code mixed version: a model that can accomodate two languages in a single utterance. Let us also consider a speech corpus $X$ consisting of languages $\{ l_1,...,l_n \}$, where each $l_i$ might comprise of multiple speakers.  Let \textit{$y_1$,...,$y_n$} denote acoustic frames in the target sequence \textit{y}  while \textit{$x_1$,...,$x_n$} denote the encoded text sequence \textit{x} from one of the languages. A typical attention based encoder decoder network such as Tacotron\citep{tacotron_original} factorizes the joint probability of acoustic frames as product of conditional probabilities. Mathematically, this can be shown as below:

\begin{equation}
P_\theta(y|x) = \Pi_{ t=1}^{t=n} P(y_t | x_1...x_m,s_t) 
\end{equation} 
where $s_t$ is a decoder state summarizing $y_1$,...$y_{t-1}$. Parameters $\theta$ of the model are set by maximizing either the log likelihood of training examples or the divergence between predicted and true target distributions. At each time step t in these models, an attention variable $a_t$ is used to denote which encoded  state of \textbf{$x_1...x_m$} aligns with \textit{$y_t$}. The most common form of attention used is soft attention, a convex combination from encoded representation of input text. It has to be noted that soft attention in such scenarios is essentially a latent deterministic variable that computes an expectation over the alignment between input and output sequences. Empirically, soft attention provides surprisingly good alignment often  correlating with human intuitions. Having said that, to synthesize speech from different languages at test time, the generative process needs to disentangle appropriate individual language attributes from observed data $X_{obs}$ and also compose them to form a coherent utterance in the voice of desired speaker. However, presence of deterministic alignment method limits the ability of models to generalize to such scenario.

%Incorporating latent alignment into Seq2Seq models by injecting stochastic latent variables allows to efficiently marginalize the likelihood of data distribution. 

On the other hand, variational attention\citep{latentalignment_variationalattention} provides a mechanism to factorize this alignment and mediate the generative process of $y$ through a stochastic variable $z$. In addition, both soft and hard attention mechanisms can be shown as special cases of ELBO\citep{latentalignment_variationalattention}. Therefore, incorporating latent stochastic variables allows us to directly optimize ELBO. In this context, model parameters are set by maximizing the log marginal likelihood of the training samples. But direct maximization of this marginal in the presence of latent variable  is often difficult due to expectation involved. To address this, a recognition network $q$ is employed to approximate the posterior probability using reparameterization. It is interesting to note that the encoder in a deterministic Seq2Seq network functions as the recognition network in latent stochastic variable models and is incentivized to search over variational distributions to improve ELBO. Intuitively, the lower bound is tight when the inferred variational distribution is closer to the true posterior of the data. This has a sense of grounding in our understanding of the task as well. Perhaps there are a set of universal phonemes, around 120, which should be enable us to speak in any language subject to the phonotactic constraints of the language. Having such prior information greatly reduces the model size as opposed to naively using a combination of all phones from all the languages to build a polyglot model. 


\subsection{Analysis of role of priors in Latent Stochastic Models}
\label{analysis}



The choice of priors plays a significant role in optimization within latent stochastic models. In this subsection, we present an analysis to show that priors control the disentanglement of causal factors of variation in such models. Let us consider the ELBO being optimized in a VAE:

\begin{equation}
E_{q_{\phi}(z|x,c)}[log p_{\theta} (x|c,z)] -  |D_{KL}(q_{\phi} (z|x,c) || p_{\theta} (z|c))| 
\end{equation}

where the first term is the reconstruction error while the second is the divergence between approximate and true posteriors. Here are the four phenomenon that are manifested due to choices of priors:

(1) \textit{Disentanglement or Factorization of causal factors of variation} \\
The KL divergence forces the posterior distribution output by encoder to follow an appropriate prior about the data generation process. Typically, prior space is assumed to be continuous distribution and a unit Gaussian. The global optimum value for the divergence in such cases is 0 and is reached only when both the distributions exactly match each other. Since the prior information about the data generation process typically involves some causal factors of variation of the data, this naturally is assumed to translate to a constraint on the encoder to track such factors. Thus, such models have potential to disentangle or factorize the causal factors of variation in the distribution.

(2) \textit{Marginalization of Nuisance Factors of Variation} \\
It has to be noted that during training optimization is performed in expectation over minibatches. Therefore, the expectation of KL divergence can be rewritten as related to the amount of mutual information between the latent representation and the data distribution \citep{pixelgan_autoencoder}. As this divergence decreases, the amount of information the encoder can place in the latent space also decreases. As a result, encoder is forced to discard some nuisance factors that may not have contributed to the generation of data. Thus, KL divergence also forces the model to marginalize the nuisance variables.

%Thus, the KL divergence forces the encoder network to disentangle the causal factors - akin to the basis vectors - of variation in the data and ignore the nuisance factors that may have contributed to the generation of data. 

(3) \textit{Posterior Collapse due to simple priors} \\
Consider the scenario where the prior is too simplistic, such as the aforementioned unit normal distribution. In such cases, the model is incentivized to force the posterior distribution to closely follow the Gaussian distribution \citep{lossy_vae}. Typically the decoders in variational models are implemented using universal approximators such as RNNs. In the context of a TTS systems, decoder segment of the acoustic model along with the neural vocoder act as the decoders. Since such decoders are very powerful, they are able to learn or ignore the priors about data distribution themselves and hence marginalize out the latent representation input from the encoder. In other words, the prediction of next sample is based solely on the marginal distribution at the current timestep which can be implemented by learning a dictionary per time step. Therefore, the encoder is no longer forced to track the causal factors of variation in the data. This is referred to as posterior collapse or mode collapse. 

(4) \textit{Loss of output fidelity due to complex priors} \\
A reasonable and intuitive solution to posterior collapse is making the prior space more complex thereby pressurizing the posterior distribution to track the prior space more closely. For instance, \citep{beta_analysis} attempt to accomplish this by adding a hyperparameter $\beta$ to promote disentanglement and gradually increasing channel capacity, something that increases loss. However, it has to be noted that simply making the prior distribution arbitrarily complex also perhaps leads to unreasonable constraints on the decoder. For instance, in scenarios that have categorical distribution as their output (tasks such as language modeling, machine translation, image captioning among others) it is unintuitive to assume that the true prior that generates latent distribution is a Gaussian when the likelihood is based on discrete sequential data in such tasks. Having such strong priors directly affects the reconstruction ability in these models.

Therefore, priors in latent stochastic models play a significant role in the optimization and facilitate disentanglement of causal factors of variation on the one hand, as well as help the ability of the model to reconstruct the data distribution on the other. Having this knowledge enables us to engineer various components to tune the model behavior as per our requirements. 


